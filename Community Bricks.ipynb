{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c1291dd-a8b8-4003-a31b-6ec5198308f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install PyPDF2 langchain transformers sentencepiece\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import LlamaTokenizer\n",
    "from pyspark.sql.functions import col, explode\n",
    "import pandas as pd\n",
    "\n",
    "# Function to split text into chunks\n",
    "def split_text_into_chunks(text: str, chunk_size: int, chunk_overlap: int) -> list:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2077b6a-400d-4e36-8145-6f8251ee786e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install PyPDF2 langchain transformers sentencepiece\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import LlamaTokenizer\n",
    "from pyspark.sql.functions import col, explode\n",
    "import pandas as pd\n",
    "\n",
    "# Function to split text into chunks\n",
    "def split_text_into_chunks(text: str, chunk_size: int, chunk_overlap: int) -> list:\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.split_text(text)\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95b7383-3123-4fae-a0ac-c2eb95274c6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "airbnb_raw = spark.sql(\"\"\"\n",
    "                       select * from sample_data.bright_initiative.airbnb_properties_information\n",
    "                       where lower(location) like '%united states%'\n",
    "                        and lower(location) like '%california%'\n",
    "                       \"\"\")\n",
    "\n",
    "display(airbnb_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d652ad0-7796-4fab-bc0c-efacb80f3439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(airbnb_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91fd09c0-87c7-4ea8-bd61-4f1ebfe39980",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define a UDF to tokenize and combine the description\n",
    "def tokenize_and_combine(description):\n",
    "    if description is None:\n",
    "        return \"\"\n",
    "    tokens = tokenizer.tokenize(description)\n",
    "    combined_string = \" \".join(tokens)\n",
    "    return combined_string\n",
    "\n",
    "# Register the UDF\n",
    "combine_udf = udf(tokenize_and_combine, StringType())\n",
    "\n",
    "# Apply the UDF to the description column\n",
    "airbnb_combined = airbnb_raw.withColumn(\n",
    "    \"combined_description\",\n",
    "    combine_udf(col(\"description\"))\n",
    ")\n",
    "\n",
    "display(airbnb_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f707ef6-7724-4765-b779-a3fa814857c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Define a function to split text into chunks with overlap\n",
    "def split_text_into_chunks(text, chunk_size=500, overlap=50):\n",
    "    if text is None:\n",
    "        return []\n",
    "    chunks = []\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        chunks.append(text[i:i + chunk_size])\n",
    "    return chunks\n",
    "\n",
    "# Register the UDF\n",
    "split_chunks_udf = udf(lambda text: split_text_into_chunks(text, 500, 50), ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to the combined_description column\n",
    "airbnb_chunked = airbnb_combined.withColumn(\n",
    "    \"chunked_description\",\n",
    "    split_chunks_udf(col(\"combined_description\"))\n",
    ")\n",
    "\n",
    "display(airbnb_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b49db4f5-ec48-4b24-8b5a-086314295413",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# Explode the chunked_description column\n",
    "airbnb_exploded = airbnb_chunked.withColumn(\n",
    "    \"exploded_chunk\",\n",
    "    explode(col(\"chunked_description\"))\n",
    ")\n",
    "\n",
    "display(airbnb_exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02669904-cb37-401c-8e8e-3cbc7f863939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write the DataFrame to the access_assist.curated table\n",
    "airbnb_exploded.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"access_assist.curated.airbnb_exploded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a8f687b-9f1f-4878-a527-5719caa3a2ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select the first 2000 rows\n",
    "airbnb_exploded_limited = airbnb_exploded.limit(2000)\n",
    "\n",
    "# Write the limited DataFrame to the access_assist.curated table\n",
    "airbnb_exploded.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"access_assist.curated.airbnb_exploded2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edb77e16-eb8b-4bd6-893b-45b1a485b120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE `access_assist`.`curated`.`airbnb_exploded` SET TBLPROPERTIES (delta.enableChangeDataFeed = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4008f20-467c-4176-aa53-8de315751f34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "%pip install databricks-vectorsearch\n",
    "%pip install -U langchain-community\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain.vectorstores import DatabricksVectorSearch\n",
    "from langchain.embeddings import DatabricksEmbeddings\n",
    "\n",
    "vs_endpoint_name = \"community_bricks_endpoint\"\n",
    "vs_index_full_name = \"access_assist.curated.airbnb_exploded_vc_idex\"\n",
    "\n",
    "embedding_model = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\n",
    "\n",
    "def get_retriever(persist_dir: str = None):\n",
    "    vsc = VectorSearchClient()\n",
    "    vs_index = vsc.get_index(vs_endpoint_name, vs_index_full_name)\n",
    "    vectorstore = DatabricksVectorSearch(\n",
    "        vs_index, text_column=\"__db_exploded_chunk_vector\", embedding=embedding_model\n",
    "    )\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "vectorstore = get_retriever()\n",
    "similar_documents = vectorstore.invoke(\"\"\"\n",
    "                                       I am looking for an accessible hotel for disabled people with wheelchair access, an elevator, and rooms equipped with grab bars in the bathroom. It should also be close to public transportation and have accessible parking.\n",
    "                                       \"\"\")\n",
    "print(f\"Relevant documents: {similar_documents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da25815-6bb0-454d-a9cc-1abf61c00e1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Ensure the Spark session is initialized at the start of the notebook\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Install necessary packages\n",
    "%pip install numpy==1.21.6\n",
    "%pip install databricks-vectorsearch\n",
    "%pip install -U langchain-community\n",
    "dbutils.library.restartPython()\n",
    "\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain.vectorstores import DatabricksVectorSearch\n",
    "from langchain.embeddings import DatabricksEmbeddings\n",
    "\n",
    "vs_endpoint_name = \"community_bricks_endpoint\"\n",
    "vs_index_full_name = \"access_assist.curated.airbnb_exploded_vc_idex\"\n",
    "\n",
    "embedding_model = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")\n",
    "\n",
    "def get_retriever(persist_dir: str = None):\n",
    "    vsc = VectorSearchClient()\n",
    "    vs_index = vsc.get_index(vs_endpoint_name, vs_index_full_name)\n",
    "    vectorstore = DatabricksVectorSearch(\n",
    "        vs_index, text_column=\"__db_exploded_chunk_vector\", embedding=embedding_model\n",
    "    )\n",
    "    return vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "vectorstore = get_retriever()\n",
    "similar_documents = vectorstore.invoke(\"\"\"\n",
    "                                       I am looking for an accessible hotel for disabled people with wheelchair access, an elevator, and rooms equipped with grab bars in the bathroom. It should also be close to public transportation and have accessible parking.\n",
    "                                       \"\"\")\n",
    "print(f\"Relevant documents: {similar_documents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c56d67a0-891f-4bae-83fb-c59218b144bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.21.0from langchain.chat_models import ChatDatabricks\n",
    "\n",
    "\n",
    "chat_model = ChatDatabricks(endpoint=\"databricks-llama-4-maverick\", max_tokens=300)\n",
    "print(f\"Test chat model: {chat_model.invoke('What is Generative AI?')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6b2a47e-6086-4042-8981-d2e1025ff5f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatDatabricks\n",
    "\n",
    "TEMPLATE = \"\"\"You are an assistant trying to find accessible places to stay .Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=TEMPLATE,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=get_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1883ea15-428c-457d-8bc2-308966b6fcf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "question = {\"query\": \"How does Generative AI impact humans\"}\n",
    "answer = chain.invoke(question)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8581948474509887,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Community Bricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
